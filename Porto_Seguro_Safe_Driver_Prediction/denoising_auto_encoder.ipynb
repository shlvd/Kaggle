{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from path import Path\n",
    "import gc\n",
    "import optuna\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from scipy.special import erfinv\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.metrics import AUC\n",
    "from tensorflow.keras.utils import get_custom_objects\n",
    "from tensorflow.keras.layers import Activation, LeakyReLU\n",
    "get_custom_objects().update({'leaky-relu': Activation(LeakyReLU(alpha=0.2))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def gpu_cleanup(objects):\n",
    "    if objects:\n",
    "        del(objects)\n",
    "    K.clear_session()\n",
    "    gc.collect()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Config:\n",
    "    input_path = Path('../input/porto-seguro-safe-driver-prediction')\n",
    "    dae_batch_size = 128\n",
    "    dae_num_epoch = 50\n",
    "    dae_architecture = [1500, 1500, 1500]\n",
    "    reuse_autoencoder = False\n",
    "    batch_size = 128\n",
    "    num_epoch = 150\n",
    "    units = [64, 32]\n",
    "    input_dropout=0.06\n",
    "    dropout=0.08\n",
    "    regL2=0.09\n",
    "    activation='selu'\n",
    "\n",
    "    cv_folds = 5\n",
    "    nas = False\n",
    "    random_state = 0\n",
    "\n",
    "config = Config()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train = pd.read_csv(config.input_path / 'train.csv', index_col='id')\n",
    "test = pd.read_csv(config.input_path / 'test.csv', index_col='id')\n",
    "submission = pd.read_csv(config.input_path / 'sample_submission.csv', index_col='id')\n",
    "calc_features = [feat for feat in train.columns if \"_calc\" in feat]\n",
    "cat_features = [feat for feat in train.columns if \"_cat\" in feat]\n",
    "target = train[\"target\"]\n",
    "train = train.drop(\"target\", axis=\"columns\")\n",
    "train = train.drop(calc_features, axis=\"columns\")\n",
    "test = test.drop(calc_features, axis=\"columns\")\n",
    "train = pd.get_dummies(train, columns=cat_features)\n",
    "test = pd.get_dummies(test, columns=cat_features)\n",
    "assert((train.columns==test.columns).all())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Applying GaussRank to columns: \", end='')\n",
    "to_normalize = list()\n",
    "for k, col in enumerate(train.columns):\n",
    "    if '_bin' not in col and '_cat' not in col and '_missing' not in col:\n",
    "        to_normalize.append(col)\n",
    "print(to_normalize)\n",
    "def to_gauss(x): return np.sqrt(2) * erfinv(x)\n",
    "def normalize(data, norm_cols):\n",
    "    n = data.shape[0]\n",
    "    for col in norm_cols:\n",
    "        sorted_idx = data[col].sort_values().index.tolist()\n",
    "        uniform = np.linspace(start=-0.99, stop=0.99, num=n)\n",
    "        normal = to_gauss(uniform)\n",
    "        normalized_col = pd.Series(index=sorted_idx, data=normal)\n",
    "        data[col] = normalized_col\n",
    "    return data\n",
    "train = normalize(train, to_normalize)\n",
    "test = normalize(test, to_normalize)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "features = train.columns\n",
    "train_index = train.index\n",
    "test_index = test.index\n",
    "train = train.values.astype(np.float32)\n",
    "test = test.values.astype(np.float32)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_keras_history(history, measures):\n",
    "    rows = len(measures) // 2 + len(measures) % 2\n",
    "    fig, panels = plt.subplots(rows, 2, figsize=(15, 5))\n",
    "    plt.subplots_adjust(top = 0.99, bottom=0.01,\n",
    "                        hspace=0.4, wspace=0.2)\n",
    "    try:\n",
    "        panels = [item for sublist in panels for item in sublist]\n",
    "    except:\n",
    "        pass\n",
    "    for k, measure in enumerate(measures):\n",
    "        panel = panels[k]\n",
    "        panel.set_title(measure + ' history')\n",
    "        panel.plot(history.epoch, history.history[measure],\n",
    "                   label=\"Train \"+measure)\n",
    "        try:\n",
    "            panel.plot(history.epoch,\n",
    "                       history.history[\"val_\"+measure],\n",
    "                       label=\"Validation \"+measure)\n",
    "        except:\n",
    "            pass\n",
    "        panel.set(xlabel='epochs', ylabel=measure)\n",
    "        panel.legend()\n",
    "\n",
    "    plt.show(fig)\n",
    "from numba import jit\n",
    "@jit\n",
    "def eval_gini(y_true, y_pred):\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_true = y_true[np.argsort(y_pred)]\n",
    "    ntrue = 0\n",
    "    gini = 0\n",
    "    delta = 0\n",
    "    n = len(y_true)\n",
    "    for i in range(n-1, -1, -1):\n",
    "        y_i = y_true[i]\n",
    "        ntrue += y_i\n",
    "        gini += y_i * delta\n",
    "        delta += 1 - y_i\n",
    "    gini = 1 - 2 * gini / (ntrue * (n - ntrue))\n",
    "    return gini"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def batch_generator(x, batch_size, shuffle=True, random_state=None):\n",
    "    batch_index = 0\n",
    "    n = x.shape[0]\n",
    "    while True:\n",
    "        if batch_index == 0:\n",
    "            index_array = np.arange(n)\n",
    "            if shuffle:\n",
    "                np.random.seed(seed=random_state)\n",
    "                index_array = np.random.permutation(n)\n",
    "        current_index = (batch_index * batch_size) % n\n",
    "        if n >= current_index + batch_size:\n",
    "            current_batch_size = batch_size\n",
    "            batch_index += 1\n",
    "        else:\n",
    "            current_batch_size = n - current_index\n",
    "            batch_index = 0\n",
    "        batch = x[index_array[current_index: current_index + current_batch_size]]\n",
    "        yield batch"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def mixup_generator(X, batch_size, swaprate=0.15, shuffle=True, random_state=None):\n",
    "    if random_state is None:\n",
    "        random_state = np.randint(0, 999)\n",
    "    num_features = X.shape[1]\n",
    "    num_swaps = int(num_features * swaprate)\n",
    "    generator_a = batch_generator(X, batch_size, shuffle,\n",
    "                                  random_state)\n",
    "    generator_b = batch_generator(X, batch_size, shuffle,\n",
    "                                  random_state + 1)\n",
    "    while True:\n",
    "        batch = next(generator_a)\n",
    "        mixed_batch = batch.copy()\n",
    "        effective_batch_size = batch.shape[0]\n",
    "        alternative_batch = next(generator_b)\n",
    "        assert((batch != alternative_batch).any())\n",
    "        for i in range(effective_batch_size):\n",
    "            swap_idx = np.random.choice(num_features, num_swaps,\n",
    "                                        replace=False)\n",
    "            mixed_batch[i, swap_idx] = alternative_batch[i, swap_idx]\n",
    "        yield (mixed_batch, batch)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_DAE(X, architecture=[1500, 1500, 1500]):\n",
    "    features = X.shape[1]\n",
    "    inputs = Input((features,))\n",
    "    for i, nodes in enumerate(architecture):\n",
    "        layer = Dense(nodes, activation='relu',\n",
    "                      use_bias=False, name=f\"code_{i+1}\")\n",
    "        if i==0:\n",
    "            x = layer(inputs)\n",
    "        else:\n",
    "            x = layer(x)\n",
    "        x = BatchNormalization()(x)\n",
    "    outputs = Dense(features, activation='linear')(x)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(optimizer='adam', loss='mse',\n",
    "                  metrics=['mse', 'mae'])\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def extract_dae_features(autoencoder, X, layers=[3]):\n",
    "    data = []\n",
    "    for layer in layers:\n",
    "        if layer == 0:\n",
    "            data.append(X)\n",
    "        else:\n",
    "            get_layer_output = Model([autoencoder.layers[0].input],\n",
    "                                     [autoencoder.layers[layer].output])\n",
    "            layer_output = get_layer_output.predict(X,\n",
    "                                                    batch_size=128)\n",
    "            data.append(layer_output)\n",
    "    data = np.hstack(data)\n",
    "    return data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def autoencoder_fitting(X_train, X_valid, filename='dae',\n",
    "                        random_state=None, suppress_output=False):\n",
    "    if suppress_output:\n",
    "        verbose = 0\n",
    "    else:\n",
    "        verbose = 2\n",
    "        print(\"Fitting a denoising autoencoder\")\n",
    "    tf.random.set_seed(seed=random_state)\n",
    "    generator = mixup_generator(X_train,\n",
    "                                batch_size=config.dae_batch_size,\n",
    "                                swaprate=0.15,\n",
    "                                random_state=config.random_state)\n",
    "\n",
    "    dae = get_DAE(X_train, architecture=config.dae_architecture)\n",
    "    steps_per_epoch = np.ceil(X_train.shape[0] /\n",
    "                              config.dae_batch_size)\n",
    "    early_stopping = EarlyStopping(monitor='val_mse',\n",
    "                                   mode='min',\n",
    "                                   patience=5,\n",
    "                                   restore_best_weights=True,\n",
    "                                   verbose=0)\n",
    "    history = dae.fit(generator,\n",
    "                      steps_per_epoch=steps_per_epoch,\n",
    "                      epochs=config.dae_num_epoch,\n",
    "                      validation_data=(X_valid, X_valid),\n",
    "                      callbacks=[early_stopping],\n",
    "                      verbose=verbose)\n",
    "    if not suppress_output: plot_keras_history(history,\n",
    "                                               measures=['mse', 'mae'])\n",
    "    dae.save(filename)\n",
    "    return dae"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def dense_blocks(x, units, activation, regL2, dropout):\n",
    "    kernel_initializer = keras.initializers.RandomNormal(mean=0.0,\n",
    "                                                         stddev=0.1, seed=config.random_state)\n",
    "    for k, layer_units in enumerate(units):\n",
    "        if regL2 > 0:\n",
    "            x = Dense(layer_units, activation=activation,\n",
    "                      kernel_initializer=kernel_initializer,\n",
    "                      kernel_regularizer=l2(regL2))(x)\n",
    "        else:\n",
    "            x = Dense(layer_units,\n",
    "                      kernel_initializer=kernel_initializer,\n",
    "                      activation=activation)(x)\n",
    "        if dropout > 0:\n",
    "            x = Dropout(dropout)(x)\n",
    "    return x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def dnn_model(dae, units=[4500, 1000, 1000],\n",
    "              input_dropout=0.1, dropout=0.5,\n",
    "              regL2=0.05,\n",
    "              activation='relu'):\n",
    "\n",
    "    inputs = dae.get_layer(\"code_2\").output\n",
    "    if input_dropout > 0:\n",
    "        x = Dropout(input_dropout)(inputs)\n",
    "    else:\n",
    "        x = tf.keras.layers.Layer()(inputs)\n",
    "    x = dense_blocks(x, units, activation, regL2, dropout)\n",
    "    outputs = Dense(1, activation='sigmoid')(x)\n",
    "    model = Model(inputs=dae.input, outputs=outputs)\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "                  loss=keras.losses.binary_crossentropy,\n",
    "                  metrics=[AUC(name='auc')])\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def model_fitting(X_train, y_train, X_valid, y_valid, autoencoder,\n",
    "                  filename, random_state=None, suppress_output=False):\n",
    "    if suppress_output:\n",
    "        verbose = 0\n",
    "    else:\n",
    "        verbose = 2\n",
    "        print(\"Fitting model\")\n",
    "    early_stopping = EarlyStopping(monitor='val_auc',\n",
    "                                   mode='max',\n",
    "                                   patience=10,\n",
    "                                   restore_best_weights=True,\n",
    "                                   verbose=0)\n",
    "    rlrop = ReduceLROnPlateau(monitor='val_auc',\n",
    "                              mode='max',\n",
    "                              patience=2,\n",
    "                              factor=0.75,\n",
    "                              verbose=0)\n",
    "\n",
    "    tf.random.set_seed(seed=random_state)\n",
    "    model = dnn_model(autoencoder,\n",
    "                      units=config.units,\n",
    "                      input_dropout=config.input_dropout,\n",
    "                      dropout=config.dropout,\n",
    "                      regL2=config.regL2,\n",
    "                      activation=config.activation)\n",
    "\n",
    "    history = model.fit(X_train, y_train,\n",
    "                        epochs=config.num_epoch,\n",
    "                        batch_size=config.batch_size,\n",
    "                        validation_data=(X_valid, y_valid),\n",
    "                        callbacks=[early_stopping, rlrop],\n",
    "                        shuffle=True,\n",
    "                        verbose=verbose)\n",
    "    model.save(filename)\n",
    "\n",
    "    if not suppress_output:\n",
    "        plot_keras_history(history, measures=['loss', 'auc'])\n",
    "    return model, history"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if config.nas is True:\n",
    "    def evaluate():\n",
    "        metric_evaluations = list()\n",
    "        skf = StratifiedKFold(n_splits=config.cv_folds, shuffle=True, random_state=config.random_state)\n",
    "        for k, (train_idx, valid_idx) in enumerate(skf.split(train, target)):\n",
    "\n",
    "            X_train, y_train = train[train_idx, :], target[train_idx]\n",
    "            X_valid, y_valid = train[valid_idx, :], target[valid_idx]\n",
    "            if config.reuse_autoencoder:\n",
    "                autoencoder = load_model(f\"./dae_fold_{k}\")\n",
    "            else:\n",
    "                autoencoder = autoencoder_fitting(X_train, X_valid,\n",
    "                                                  filename=f'./dae_fold_{k}',\n",
    "                                                  random_state=config.random_state,\n",
    "                                                  suppress_output=True)\n",
    "\n",
    "            model, _ = model_fitting(X_train, y_train, X_valid, y_valid,\n",
    "                                     autoencoder=autoencoder,\n",
    "                                     filename=f\"dnn_model_fold_{k}\",\n",
    "                                     random_state=config.random_state,\n",
    "                                     suppress_output=True)\n",
    "\n",
    "            val_preds = model.predict(X_valid, batch_size=128, verbose=0)\n",
    "            best_score = eval_gini(y_true=y_valid, y_pred=np.ravel(val_preds))\n",
    "            metric_evaluations.append(best_score)\n",
    "\n",
    "            gpu_cleanup([autoencoder, model])\n",
    "\n",
    "        return np.mean(metric_evaluations)\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            'first_layer': trial.suggest_categorical(\"first_layer\", [8, 16, 32, 64, 128, 256, 512]),\n",
    "            'second_layer': trial.suggest_categorical(\"second_layer\", [0, 8, 16, 32, 64, 128, 256]),\n",
    "            'third_layer': trial.suggest_categorical(\"third_layer\", [0, 8, 16, 32, 64, 128, 256]),\n",
    "            'input_dropout': trial.suggest_float(\"input_dropout\", 0.0, 0.5),\n",
    "            'dropout': trial.suggest_float(\"dropout\", 0.0, 0.5),\n",
    "            'regL2': trial.suggest_uniform(\"regL2\", 0.0, 0.1),\n",
    "            'activation': trial.suggest_categorical(\"activation\", ['relu', 'leaky-relu', 'selu'])\n",
    "        }\n",
    "        config.units = [nodes for nodes in [params['first_layer'], params['second_layer'], params['third_layer']] if nodes > 0]\n",
    "        config.input_dropout = params['input_dropout']\n",
    "        config.dropout = params['dropout']\n",
    "        config.regL2 = params['regL2']\n",
    "        config.activation = params['activation']\n",
    "\n",
    "        return evaluate()\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=60)\n",
    "    print(\"Best Gini Normalized Score\", study.best_value)\n",
    "    print(\"Best parameters\", study.best_params)\n",
    "    config.units = [nodes for nodes in [study.best_params['first_layer'], study.best_params['second_layer'], study.best_params['third_layer']] if nodes > 0]\n",
    "    config.input_dropout = study.best_params['input_dropout']\n",
    "    config.dropout = study.best_params['dropout']\n",
    "    config.regL2 = study.best_params['regL2']\n",
    "    config.activation = study.best_params['activation']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "preds = np.zeros(len(test))\n",
    "oof = np.zeros(len(train))\n",
    "metric_evaluations = list()\n",
    "skf = StratifiedKFold(n_splits=config.cv_folds, shuffle=True, random_state=config.random_state)\n",
    "for k, (train_idx, valid_idx) in enumerate(skf.split(train, target)):\n",
    "    print(f\"CV fold {k}\")\n",
    "\n",
    "    X_train, y_train = train[train_idx, :], target[train_idx]\n",
    "    X_valid, y_valid = train[valid_idx, :], target[valid_idx]\n",
    "    if config.reuse_autoencoder:\n",
    "        print(\"restoring previously trained dae\")\n",
    "        autoencoder = load_model(f\"./dae_fold_{k}\")\n",
    "    else:\n",
    "        autoencoder = autoencoder_fitting(X_train, X_valid,\n",
    "                                          filename=f'./dae_fold_{k}',\n",
    "                                          random_state=config.random_state)\n",
    "\n",
    "    model, history = model_fitting(X_train, y_train, X_valid, y_valid,\n",
    "                                   autoencoder=autoencoder,\n",
    "                                   filename=f\"dnn_model_fold_{k}\",\n",
    "                                   random_state=config.random_state)\n",
    "\n",
    "    val_preds = model.predict(X_valid, batch_size=128)\n",
    "    best_score = eval_gini(y_true=y_valid,\n",
    "                           y_pred=np.ravel(val_preds))\n",
    "    best_epoch = np.argmax(history.history['val_auc']) + 1\n",
    "    print(f\"[best epoch is {best_epoch}]\\tvalidation_0-gini_dnn: {best_score:0.5f}\\n\")\n",
    "\n",
    "    metric_evaluations.append(best_score)\n",
    "    preds += (model.predict(test, batch_size=128).ravel() /\n",
    "              skf.n_splits)\n",
    "    oof[valid_idx] = model.predict(X_valid, batch_size=128).ravel()\n",
    "    gpu_cleanup([autoencoder, model])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(f\"DNN CV normalized Gini coefficient: {np.mean(metric_evaluations):0.3f} ({np.std(metric_evaluations):0.3f})\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "submission['target'] = preds\n",
    "submission.to_csv('dnn_submission.csv')\n",
    "oofs = pd.DataFrame({'id':train_index, 'target':oof})\n",
    "oofs.to_csv('dnn_oof.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}