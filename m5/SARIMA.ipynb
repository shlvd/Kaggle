{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport seaborn as sns\nsns.set_style('whitegrid')\nsns.set_context('poster')\n\nimport matplotlib.pyplot as plt\nfrom pandas_profiling import ProfileReport\nfrom IPython.display import HTML\n\nfrom tqdm import tqdm\n\n%matplotlib inline\nplt.rcParams[\"figure.figsize\"] = (16,12)\nplt.rcParams['axes.titlesize'] = 16\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# load dataset\ntrain_sales = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/sales_train_validation.csv')\nsell_prices = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/sell_prices.csv')\ncalendar = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/calendar.csv')\nsubmission = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"#quick look at data shapes\nprint(train_sales.shape)\nprint(sell_prices.shape)\nprint(calendar.shape)\nprint(submission.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#samples of train set\ntrain_sales.sample(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#sample of prices set\nsell_prices.sample(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#samples of calendar set\ncalendar.sample(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## This code should be done on local machine due to kernel memory constraints"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"\n# Create a list comprehension for all the date columns to melt.\nd_cols = ['d_' + str(i + 1) for i in range(1913)]\n\n# Melt columns into rows so that each row is a separate and discrete entry with one target\ntidy_df = pd.melt(frame = train_sales, \n                 id_vars = ['id', 'item_id', 'cat_id', 'store_id'],\n                 var_name = 'd',\n                 value_vars = d_cols,\n                 value_name = 'sales')\n\n# This has duplicate ID's now.  We should add the date to the id to make each row unique.\nnew_ids = tidy_df['id'] + '_' + tidy_df['d']\ntidy_df['id'] = new_ids\n\n# Check this turned out ok so far.\ntidy_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Merge the prices.  \n# NOTE - For now we are aggregating on the mean price of each item.\n# TO DO: We will want to set the price with the week or run some statistics on price volatility over time.\n\nprice_means = sell_prices.groupby(['item_id']).mean()\n\n# Now, merge this and the date col\nwith_prices_df = pd.merge(left = tidy_df, right = calendar,\n                        on = 'd')\n\nwith_prices_df.head(10)\n# Let's see the results.","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"with_date_info_df = pd.merge(left = with_prices_df, right = price_means,\n                        on = 'item_id')\n\ntotal_tidy_df = with_date_info_df\ntotal_tidy_df.columns\n\n# Drop d and drop item_id (price is an informative proxy)\ntotal_tidy_df.drop(['d', 'wday', 'item_id'], axis = 1, inplace = True)\n\n# fill categorical NaNs with 0's.\ntotal_tidy_df = total_tidy_df.fillna(0)\n\nprint(with_date_info_df.iloc[0])\n\n","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Categorical encoded column helper function.\ndef categorically_encode_col(df, col):\n    encoded_df = pd.get_dummies(df[col], \n                                prefix = str(col),\n                               drop_first = False)\n\n    return encoded_df\n\ntotal_tidy_df.columns\n\n# Categorically encode the categorical columns and then drop the originals.\n# This makes them ML ready.\n\nif CREATE_TIDY_DF:\n    \n    # Categorically encode categorical columns\n    cols_to_encode = ['cat_id', 'store_id', 'weekday', 'event_type_1', 'event_type_2' ]\n    \n    for col in cols_to_encode:\n        new_cols = pd.DataFrame(categorically_encode_col(total_tidy_df, col))\n        total_tidy_df = pd.concat([total_tidy_df, new_cols], axis = 1)\n        # total_tidy_df.drop(col, inplace = True)  # Drop the un-encoded column","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"total_tidy_df.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Yearly Cycle Decompose of CA_1 store"},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.tsa.seasonal import seasonal_decompose\ndays_per_year = 365\n\ntime_series = store_sum[\"CA_1\"]\nsj_sc = seasonal_decompose(time_series, period = days_per_year)\nsj_sc.plot()\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. ## Weekly cycle decompose of CA_1 store ##"},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.tsa.seasonal import seasonal_decompose\ndays_per_week = 7\n\ntime_series = store_sum[\"CA_1\"]\nsj_sc = seasonal_decompose(time_series, period = days_per_week)\nsj_sc.plot()\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Baseline Sarima Model ##"},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"from statsmodels.tsa.statespace.sarimax import SARIMAX\n\ndef sarima_train_test(t_series, p = 2, d = 1, r = 2, NUM_TO_FORECAST = 56, do_plot_results = True):\n    NUM_TO_FORECAST = NUM_TO_FORECAST  # Similar to train test splits.\n    dates = np.arange(t_series.shape[0])\n\n    model = SARIMAX(t_series, order = (p, d, r), trend = 'c')\n    results = model.fit()\n    results.plot_diagnostics(figsize=(18, 14))\n    plt.show()\n\n    forecast = results.get_prediction(start = -NUM_TO_FORECAST)\n    mean_forecast = forecast.predicted_mean\n    conf_int = forecast.conf_int()\n\n    print(mean_forecast.shape)\n\n    # Plot the forecast\n    plt.figure(figsize=(14,16))\n    plt.plot(dates[-NUM_TO_FORECAST:],\n            mean_forecast.values,\n            color = 'red',\n            label = 'forecast')\n\n\n    plt.plot(dates[-NUM_TO_FORECAST:],\n            t_series.iloc[-NUM_TO_FORECAST:],\n            color = 'blue',\n            label = 'actual')\n    plt.legend()\n    plt.title('Predicted vs. Actual Values')\n    plt.show()\n    \n    residuals = results.resid\n    mae_sarima = np.mean(np.abs(residuals))\n    print('Mean absolute error: ', mae_sarima)\n    print(results.summary())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sarima_train_test(time_series)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nsarima_preds = pd.read_csv('/kaggle/input/sarima_submission.csv')\nsarima_preds[sarima_preds < 0] = 0  # Convert all negative numbers into 0.\nsarima_preds['id']= submission['id']\n\nsubmission_df = sarima_preds\n\n#Cleaning\nsubmission_df = submission_df.iloc[:,:29]\nsubmission_df = submission_df.drop(['Unnamed: 0'], axis = 1)\nsubmission_df.index = submission_file['id']\nsubmission_df.reset_index(inplace = True)\nsubmission_df.columns = submission_file.columns\nsubmission_df.head()\n\nsarima_df = submission_df.copy()\nsarima_df.to_csv('SARIMA.csv', index = False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}